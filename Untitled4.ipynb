{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3cab31",
   "metadata": {},
   "source": [
    "# HW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c996f",
   "metadata": {},
   "source": [
    " **1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc23c6",
   "metadata": {},
   "source": [
    "A **Classification Decision Tree** addresses problems where the goal is to predict a categorical outcome (i.e., the class or category) based on input features. It is particularly useful in situations where the data is structured and the relationships between variables are non-linear. Decision trees provide interpretable models that are easy to visualize and understand.\n",
    "\n",
    "### Examples of Real-World Applications:\n",
    "1. **Medical Diagnosis**  \n",
    "   - Problem: Classifying whether a patient has a particular disease (e.g., diabetes) based on symptoms, test results, and demographic information.  \n",
    "   - Usefulness: Doctors can understand the decision-making process to explain it to patients.\n",
    "\n",
    "2. **Customer Segmentation in Marketing**  \n",
    "   - Problem: Categorizing customers into groups (e.g., likely to buy, unlikely to buy) based on features like age, income, browsing history, and past purchases.  \n",
    "   - Usefulness: Helps businesses target specific customer groups with tailored marketing strategies.\n",
    "\n",
    "3. **Spam Email Detection**  \n",
    "   - Problem: Classifying emails as \"spam\" or \"not spam\" based on features like keywords, sender information, and attachment type.  \n",
    "   - Usefulness: Improves email filtering systems.\n",
    "\n",
    "4. **Fraud Detection**  \n",
    "   - Problem: Identifying fraudulent transactions (e.g., credit card fraud) based on transaction amount, location, time, and user behavior.  \n",
    "   - Usefulness: Enhances security and prevents financial losses.\n",
    "\n",
    "5. **Loan Approval in Banking**  \n",
    "   - Problem: Predicting whether a loan applicant should be approved or denied based on features like credit score, income, employment history, and debt-to-income ratio.  \n",
    "   - Usefulness: Provides a clear rationale for decision-making in financial institutions.\n",
    "\n",
    "6. **Predicting Student Outcomes**  \n",
    "   - Problem: Predicting whether a student will pass, fail, or drop out based on attendance, grades, participation, and background data.  \n",
    "   - Usefulness: Enables early intervention strategies in education.\n",
    "\n",
    "### Why Use Decision Trees?  \n",
    "- **Transparency:** Easy to visualize and interpret compared to black-box models like neural networks.  \n",
    "- **Flexibility:** Handles both categorical and continuous input features.  \n",
    "- **No Preprocessing Needed:** Can work directly with raw data without scaling or normalization.  \n",
    "- **Non-Parametric:** Does not assume any specific distribution for the data.\n",
    "\n",
    "Classification Decision Trees are an intuitive choice when interpretability and clarity of decision-making are critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d9ec5",
   "metadata": {},
   "source": [
    "Classification Decision Tree: Predicts classes by splitting data based on feature values, ideal for non-linear, categorical problems.\n",
    "Multiple Linear Regression: Predicts numeric values using a linear equation, best for continuous regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fc8a7",
   "metadata": {},
   "source": [
    "history :https://chatgpt.com/share/673fadc9-21a4-8003-8afe-eb7381723624"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decb103",
   "metadata": {},
   "source": [
    " **2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e71afc",
   "metadata": {},
   "source": [
    "Accuracy use in the case of General performance in balanced datasets.which focus on Overall correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94006ea",
   "metadata": {},
   "source": [
    "Sensitivity is uesd in the case of \tHigh stakes for missing true positives (FN).which focus on Capturing as many positives as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2794c7",
   "metadata": {},
   "source": [
    "Specificityis used in High stakes for false positives (FP) which focus on \tMinimizing false alarms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5619ce",
   "metadata": {},
   "source": [
    "Precision is used in the case Critical to avoid false positives (FP) which focus on  Ensuring positive predictions are reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c80661",
   "metadata": {},
   "source": [
    "**3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Remove the specified columns: Weight_oz, Width, and Height\n",
    "ab_reduced = ab.drop(columns=['Weight_oz', 'Width', 'Height'])\n",
    "\n",
    "# Drop rows with NaN entries\n",
    "ab_reduced_noNaN = ab_reduced.dropna()\n",
    "\n",
    "# Convert 'Pub year' and 'NumPages' to int type\n",
    "ab_reduced_noNaN['Pub year'] = ab_reduced_noNaN['Pub year'].astype(int)\n",
    "ab_reduced_noNaN['NumPages'] = ab_reduced_noNaN['NumPages'].astype(int)\n",
    "\n",
    "# Convert 'Hard_or_Paper' to category type\n",
    "ab_reduced_noNaN['Hard_or_Paper'] = ab_reduced_noNaN['Hard_or_Paper'].astype('category')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(ab_reduced_noNaN.info())\n",
    "\n",
    "# Perform some initial EDA to explore the dataset\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(ab_reduced_noNaN.describe())\n",
    "\n",
    "# Show the first few rows of the cleaned dataset\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(ab_reduced_noNaN.head())\n",
    "\n",
    "# Check for the unique categories in 'Hard_or_Paper'\n",
    "print(\"\\nUnique values in 'Hard_or_Paper' column:\")\n",
    "print(ab_reduced_noNaN['Hard_or_Paper'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59c473",
   "metadata": {},
   "source": [
    "**4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915844a",
   "metadata": {},
   "source": [
    "next two steps are \n",
    "model = DecisionTreeClassifier()   and \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066a77c",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named 'df', and it contains 'List Price' and 'BookType' columns\n",
    "# Let's filter the dataset for the relevant columns and prepare the data\n",
    "X = df[['List Price']]  # Features (List Price)\n",
    "y = df['BookType']  # Target (BookType, which is assumed to be 'Hardcover' or 'Paperback')\n",
    "\n",
    "# Train the classifier with max_depth=2\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(12,8))  # Optional: to make the plot larger\n",
    "plot_tree(clf, feature_names=['List Price'], class_names=clf.classes_, filled=True, rounded=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f805f2",
   "metadata": {},
   "source": [
    "What the tree tells you:\n",
    "The tree will make decisions about whether a book is a hardcover or paperback based on the List Price.\n",
    "Each node in the tree will show the feature value (List Price) and how it splits the data based on the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a468616",
   "metadata": {},
   "source": [
    "history : https://chatgpt.com/share/673fb1e0-0bd4-8003-b843-0e0809e26ac2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78a13d",
   "metadata": {},
   "source": [
    "**5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafcf33e",
   "metadata": {},
   "source": [
    "To visualize the classification decision tree for the specified features (`'NumPages'`, `'Thick'`, and `'List Price'`), we'll follow these steps:\n",
    "\n",
    "1. **Train the model**: Fit the decision tree classifier (`clf2`) using the specified features and set `max_depth=4`.\n",
    "2. **Visualize the tree**: Use `plot_tree` from `sklearn.tree` to generate a plot of the trained decision tree.\n",
    "3. **Explain predictions**: After the tree is visualized, I will explain how predictions are made.\n",
    "\n",
    "Hereâ€™s the Python code for the process:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Assuming ab_reduced_noNaN is the dataset\n",
    "X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]\n",
    "y = ab_reduced_noNaN['TargetColumn']  # Replace with the actual target column\n",
    "\n",
    "# Initialize and train the classifier with max_depth set to 4\n",
    "clf2 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2.fit(X, y)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(clf2, filled=True, feature_names=['NumPages', 'Thick', 'List Price'], class_names=['Class 0', 'Class 1'], rounded=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation of how predictions are made for `clf2`:\n",
    "\n",
    "1. **Tree Structure**: The decision tree splits the data based on certain feature values (such as `NumPages`, `Thick`, or `List Price`). At each node of the tree, the model chooses the best feature to split the data, based on minimizing the impurity (like Gini impurity or entropy).\n",
    "\n",
    "2. **Prediction Process**:\n",
    "   - When making a prediction for a new data point, the decision tree follows the splits from the root to a leaf.\n",
    "   - At each node, the feature value of the data point is compared with the threshold set by the tree at that node.\n",
    "   - The path is determined by whether the feature value is greater than or less than the threshold.\n",
    "   - The process continues until a leaf node is reached. Each leaf node represents a predicted class label, and the most frequent class label among the training samples that end up in that leaf node is the predicted class for the new data point.\n",
    "\n",
    "3. **Prediction Example**: For a data point with specific values for `NumPages`, `Thick`, and `List Price`, the model navigates through the tree starting at the root and proceeds down through the nodes based on the comparisons at each node. Once it reaches a leaf, the predicted class is given by the majority class in that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a6e82",
   "metadata": {},
   "source": [
    "**6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions for clf and clf2\n",
    "clf_predictions = clf.predict(ab_reduced_noNaN_test[['NumPages', 'Thick', 'List Price']])\n",
    "clf2_predictions = clf2.predict(ab_reduced_noNaN_test[['NumPages', 'Thick', 'List Price']])\n",
    "\n",
    "# Actual outcomes (true labels) from the test set\n",
    "y_true = ab_reduced_noNaN_test['your_actual_outcome_variable']  # Replace with the actual target column\n",
    "\n",
    "# Confusion matrix for clf\n",
    "cm_clf = confusion_matrix(y_true, clf_predictions, labels=[0, 1])\n",
    "# Confusion matrix for clf2\n",
    "cm_clf2 = confusion_matrix(y_true, clf2_predictions, labels=[0, 1])\n",
    "\n",
    "# Calculate metrics for clf\n",
    "TP_clf = cm_clf[1, 1]  # True positives\n",
    "TN_clf = cm_clf[0, 0]  # True negatives\n",
    "FP_clf = cm_clf[0, 1]  # False positives\n",
    "FN_clf = cm_clf[1, 0]  # False negatives\n",
    "\n",
    "sensitivity_clf = TP_clf / (TP_clf + FN_clf)\n",
    "specificity_clf = TN_clf / (TN_clf + FP_clf)\n",
    "accuracy_clf = accuracy_score(y_true, clf_predictions)\n",
    "\n",
    "# Calculate metrics for clf2\n",
    "TP_clf2 = cm_clf2[1, 1]  # True positives\n",
    "TN_clf2 = cm_clf2[0, 0]  # True negatives\n",
    "FP_clf2 = cm_clf2[0, 1]  # False positives\n",
    "FN_clf2 = cm_clf2[1, 0]  # False negatives\n",
    "\n",
    "sensitivity_clf2 = TP_clf2 / (TP_clf2 + FN_clf2)\n",
    "specificity_clf2 = TN_clf2 / (TN_clf2 + FP_clf2)\n",
    "accuracy_clf2 = accuracy_score(y_true, clf2_predictions)\n",
    "\n",
    "# Print results\n",
    "print(f\"Metrics for clf (Decision Tree with max_depth=4):\")\n",
    "print(f\"Sensitivity: {sensitivity_clf:.4f}\")\n",
    "print(f\"Specificity: {specificity_clf:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_clf:.4f}\")\n",
    "\n",
    "print(f\"\\nMetrics for clf2 (Decision Tree with max_depth=4):\")\n",
    "print(f\"Sensitivity: {sensitivity_clf2:.4f}\")\n",
    "print(f\"Specificity: {specificity_clf2:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_clf2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3bfb2",
   "metadata": {},
   "source": [
    "**7**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071b68b",
   "metadata": {},
   "source": [
    "The differences between the two confusion matrices arise because of the different sets of features used to train the models. In the first case, only `'List Price'` is used, which may not provide enough information for the model to make accurate predictions. In contrast, the second case uses multiple features (`'NumPages'`, `'Thick'`, and `'List Price'`), which likely improves the model's ability to distinguish between the two classes (`Paper` and `Hard`), leading to better performance.\n",
    "\n",
    "The two confusion matrices for `clf` and `clf2` are likely better because they use a more informative set of features, leading to a more robust decision-making process by the classifier. By considering additional variables such as `'NumPages'` and `'Thick'`, the classifier can make more accurate predictions, reducing misclassifications and improving the overall predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3610800",
   "metadata": {},
   "source": [
    "**8**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2809069",
   "metadata": {},
   "source": [
    "To visualize feature importances for a classification decision tree in scikit-learn, you can use a bar plot to represent the relative contribution of each feature based on the `.feature_importances_` attribute. Here's how to do that for a classifier, `clf2`:\n",
    "\n",
    "### Steps:\n",
    "1. **Train the classification tree** (assumed to be already done).\n",
    "2. **Retrieve the feature importances** from `clf2` using `.feature_importances_`.\n",
    "3. **Visualize the importances** using a bar plot, where each bar corresponds to a feature.\n",
    "\n",
    "Here's an example code snippet for this:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming clf2 is the trained classification decision tree classifier\n",
    "# clf2.feature_importances_ contains the importance of each feature\n",
    "importances = clf2.feature_importances_\n",
    "\n",
    "# clf2.feature_names_in_ contains the feature names corresponding to the importances\n",
    "feature_names = clf2.feature_names_in_\n",
    "\n",
    "# Create a bar plot of feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance for Classification Decision Tree')\n",
    "plt.show()\n",
    "\n",
    "# Find the most important feature\n",
    "most_important_feature = feature_names[importances.argmax()]\n",
    "print(f\"The most important feature is: {most_important_feature}\")\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- `clf2.feature_importances_` gives an array with the importance scores of the features.\n",
    "- `clf2.feature_names_in_` gives the names of the features, so you can match each importance score to a feature name.\n",
    "- The `barh()` function is used to create a horizontal bar plot, making it easier to see which features are more important.\n",
    "- `importances.argmax()` returns the index of the feature with the highest importance, and you can use this to print the most important feature.\n",
    "\n",
    "This method gives you both a visual understanding of feature importance and the specific feature that is most influential in your model's predictions.\n",
    "\n",
    "Let me know if you need help adapting this to your specific setup!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7302c7",
   "metadata": {},
   "source": [
    "**9**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f84f3d0",
   "metadata": {},
   "source": [
    "In linear regression, the coefficients represent the change in the predicted outcome for a one-unit change in the corresponding feature, assuming all other features remain constant. In contrast, feature importances in decision trees indicate how much a feature contributes to reducing uncertainty (or impurity) in the model's predictions, with higher values reflecting greater importance in making the final prediction. While linear regression offers a direct interpretation of feature effects, decision tree feature importance is more about the feature's overall contribution to model performance rather than a simple relationship with the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64c33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b3931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021da814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae1a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8abf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794185fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d91458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aad028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9ecda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d0dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
